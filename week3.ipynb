{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXUdbxqqOoesekCTHBDHSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JARAM2021/NLP_Study/blob/Keunmo/week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abv62vN2844w"
      },
      "source": [
        "# Attention Is All You Need  \n",
        "논문링크: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)  \n",
        "\n",
        "RNN 아키텍쳐 -> Transformer 아케텍쳐 사용  \n",
        "\n",
        "- 기존 Seq2Seq 모델의 한계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5fDlUK195Fd"
      },
      "source": [
        "## 기존 Seq2Seq 모델의 한계  \n",
        "- 하나의 context vector 에 소스 문장 정보 압축.  \n",
        " -> 병목이 발생. 성능 하락.  \n",
        "  매번 소스 문장에서의 출력 전부를 입력으로 받는다면?  \n",
        "\n",
        "- 에텐션 매커니즘  \n",
        "  각 단어의 모든 히든 스테이트 값을 별도의 배열에 저장.  \n",
        "  디코더가 인코더의 모든 출력을 참고.  \n",
        "  어텐션 가중치를 사용해 각 출력이 어떤 입력 정보를 참고했는지 시각화 가능.  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSReIA3HKMgK"
      },
      "source": [
        "## Transformer  \n",
        "- RNN, CNN 전혀 사용 안함.  \n",
        "  순서정보를 위해 Positional Encoding 사용  \n",
        "- 인코더와 디코더로 구성  \n",
        "  Attention 과정을 여러 레이어에서 반복  \n",
        "  n개의 인코더 -> n개의 디코더  \n",
        "  마지막 인코더 레이어의 출력이 모든 디코더 레이어에 입력됨.  \n",
        "- 셀프 어텐션: 인코더와 디코더 모두에서 사용. 각 단어가 다른 어떤 단어와 연관성이 높은지 계산.  \n",
        "\n"
      ]
    }
  ]
}