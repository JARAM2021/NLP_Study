{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYfSy3pPX4vHqzwWSHy94E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JARAM2021/NLP_Study/blob/nykim/nykim/week5/week5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0hoRG69oqYq"
      },
      "source": [
        "# DataLoader\n",
        "- PyTorch의 Dataset과 DataLoader를 이용하여 방대한 데이터를 미니배치 단위로 처리\n",
        "- 데이터를 랜덤하게 섞어 학습의 효율성을 향상\n",
        "- 여러 개의 GPU를 사용해 병렬 처리로 학습 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpzcxOdmol3i",
        "outputId": "0bf67bc4-d6c4-4785-83dc-80f8317037d6"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgHUljpErCSJ",
        "outputId": "e9f7c751-f024-468e-91c8-55ff1aff7aa2"
      },
      "source": [
        "cd /content/gdrive/MyDrive/Colab Notebooks/2021_NLP_STUDY/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/2021_NLP_STUDY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSi8m3w8r_l6"
      },
      "source": [
        "## Kobert Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcCZiv7VsCMe",
        "outputId": "b20f8196-c43d-41b5-87c7-8e2e24eb6272"
      },
      "source": [
        "!pip install mxnet-cu101mkl\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101mkl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/3f/e33e3f92110fa5caba5e9eb052008208a33c1d5faccc7fe5312532e9aa42/mxnet_cu101mkl-1.6.0.post0-py2.py3-none-manylinux1_x86_64.whl (712.3MB)\n",
            "\u001b[K     |████████████████████████████████| 712.3MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101mkl) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101mkl) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet-cu101mkl\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101mkl-1.6.0.post0\n",
            "Collecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (20.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595684 sha256=219cba0ca23b0ee45e02b359e4bcbaef0931be3bf582851e882e4886573e281b\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnxy3ObCsVab",
        "outputId": "620a5207-0fa5-478b-8cb3-29dd94ed72a2"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-74cxtmzx\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-74cxtmzx\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-cp37-none-any.whl size=12708 sha256=dba274d3160bc325179e033a0d761b86b159a6db254062b07bb3d5d93da552b8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-69fe7phq/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY4H76xV5pYo"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0szYulcxNqq"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTTqTW-850oI",
        "outputId": "59985ed8-977f-4f97-e947-abaa0e082f70"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 7.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 33.6MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNkjJoXlxPh_",
        "outputId": "59034c64-a6eb-44ef-f681-bb2504e3f31c"
      },
      "source": [
        "# from gluonnlp.data import SentencepieceTokenizer\n",
        "import gluonnlp as nlp\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "bertmodel, vocab = get_pytorch_kobert_model()\n",
        "\n",
        "tok_path = get_tokenizer()\n",
        "tokenizer = nlp.data.BERTSPTokenizer(tok_path, vocab, lower=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWd-KA2mxhWz",
        "outputId": "88ea4406-0cb3-4a94-da22-68ee8e855123"
      },
      "source": [
        "print(tokenizer('한국어 모델을 공유합니다.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁한국', '어', '▁모델', '을', '▁공유', '합니다', '▁', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjXKZeE72mDQ"
      },
      "source": [
        "## Train/Test data splitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmTeZGHZ2phP"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# from torch.utils.data import Subset\n",
        "import pandas as pd\n",
        "\n",
        "def train_test_splitter(csv_file) :\n",
        "    dataset = pd.read_csv(csv_file)\n",
        "    # train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=split)\n",
        "    # print(len(train_idx))\n",
        "    # print(len(test_idx))\n",
        "    # train_set = Subset(dataset, train_idx)\n",
        "    # test_set = Subset(dataset, test_idx)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac7ccYOl2tAT"
      },
      "source": [
        "dataset = train_test_splitter('./week5_data/ko_train_label.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "a3SLrU0Y3NUC",
        "outputId": "5bb605d7-6d06-44b4-f6be-56574c28041f"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5403919</td>\n",
              "      <td>막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7797314</td>\n",
              "      <td>원작의 긴장감을 제대로 살려내지못했다.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9443947</td>\n",
              "      <td>별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>7448293</td>\n",
              "      <td>혹시나 그래도 카메론디아즈니까 하고봤는데...먼이런영화를..이도저도아닌 .아암튼 시...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>5824024</td>\n",
              "      <td>10점주는것들 한국영화는 1점주네. M창</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>6420437</td>\n",
              "      <td>영상도 아름답고 뭘 말하는지 알겠지만 그렇기 때문에 짜증나고 답답하다</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>6777278</td>\n",
              "      <td>영화를 왜 영화라고 하는지 모르는 애들이 나왔네.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9238637</td>\n",
              "      <td>내가 이걸보고 며칠내내 화가났네 별이 아깝 어쩜이리쓰래기가... 내용잡다한거 다섞여...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9999 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... identity_hate\n",
              "0     9976970  ...             0\n",
              "1     9045019  ...             0\n",
              "2     5403919  ...             0\n",
              "3     7797314  ...             0\n",
              "4     9443947  ...             1\n",
              "...       ...  ...           ...\n",
              "9994  7448293  ...             0\n",
              "9995  5824024  ...             0\n",
              "9996  6420437  ...             0\n",
              "9997  6777278  ...             0\n",
              "9998  9238637  ...             0\n",
              "\n",
              "[9999 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_r3jCdItW_O"
      },
      "source": [
        "## Dataset / DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5xv52b-b3rz",
        "outputId": "d23f7ca8-be1a-43d4-83e9-f309db20b718"
      },
      "source": [
        "!pip install soynlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soynlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n",
            "\r\u001b[K     |▉                               | 10kB 12.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 12.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 71kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 92kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.1)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FwETj5ub4u0"
      },
      "source": [
        "from soynlp.normalizer import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIm4vPdcrhFv"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "class HateSpeechDataset(Dataset) :\n",
        "    def __init__(self, csv_file, bert_tokenizer, max_len, pad, pair) :\n",
        "        self.transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "        self.hatespeech = pd.read_csv(csv_file)\n",
        "    \n",
        "    def __len__(self) :\n",
        "        return len(self.hatespeech)\n",
        "    \n",
        "    def __getitem__(self, idx) :\n",
        "        instance = self.hatespeech.iloc[idx]\n",
        "        sentence = transform(repeat_normalize(instance[1]))\n",
        "        label = instance[2]\n",
        "        \n",
        "        return {'sentence' : np.array(sentence, dtype=string_),\n",
        "                'label' : label}\n",
        "# class HateSpeechDataset(Dataset) :\n",
        "#     def __init__(self, dataset, bert_tokenizer, sent_idx, toxic_idx, obscene_idx, threat_idx, insult_idx, identity_hate_idx, max_len, pad, pair) :\n",
        "#         transform = nlp.data.BERTSentenceTransform(\n",
        "#             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "#         self.sentences = [transform([repeat_normalize(i[sent_idx])]) for i in dataset]\n",
        "#         self.toxic = [np.int32(i[toxic_idx]) for i in dataset]\n",
        "#         self.obscene = [np.int32(i[obscene_idx]) for i in dataset]\n",
        "#         self.threat = [np.int32(i[threat_idx]) for i in dataset]\n",
        "#         self.insult = [np.int32(i[insult_idx]) for i in dataset]\n",
        "#         self.identity_hate = [np.int32(i[identity_hate_idx]) for i in dataset]\n",
        "    \n",
        "#     def __getitem__(self, i) :\n",
        "#         return (self.sentences[i] + (self.labels[i], ))\n",
        "    \n",
        "#     def __len__(self) :\n",
        "#         return (len(self.labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iLIFVsjoJyL7",
        "outputId": "5a7c2217-05d3-4e3a-a1c8-8a0b674749f3"
      },
      "source": [
        "dataset.iloc[1][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MycoyvEf4gWr"
      },
      "source": [
        "# train_data = HateSpeechDataset(dataset, tokenizer, 1, 2, 3, 4, 5, 6, 512, True, False)\n",
        "train_data = HateSpeechDataset('./week5_data/hate_speech_binary_dataset.csv', tokenizer, 128, True, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAc9fTeD5HfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5081a6-88fb-406b-e498-e9aad03ec26b"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.HateSpeechDataset at 0x7f5aeaadcd90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmpQj2aDewZB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}