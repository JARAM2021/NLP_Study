# BERT
- 사전 훈련 언어모델
- 특정 과제를 하기 전 사전 훈련 Embedding을 통해 특정 과제의 성능을 더 좋게 할 수 있는 언어모델

BERT를 사용하지 않은 일반적인 모델링 과정은 (분류를 원하는 데이터->모델->분류) 순서이지만,
BERT를 사용하면 (관련 말뭉치->BERT->분류를 원하는 데이터->모델->분류) 순서로 이루어진다.

## BERT의 내부 동작 과정
### 1. INPUT
BERT Input : Token Embedding + Segment Embedding + Position Embedding

#### Token Embedding
- Word Piece 임베딩 방식 사용 [Word Piece](https://lovit.github.io/nlp/2018/04/02/wpm/)
  -각 Char 단위로 임베딩을 한다.
  -자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 만든다.
  -이는 이전에 자주 등장하지 않았던 단어를 모조리 'OOV'처리하여 모델링의 성능을 저하했던 'OOV'문제도 해결 할 수 있다.
  
#### Segment Embedding
- 토큰 시킨 단어들을 다시 하나의 문장으로 만드는 작업이다.
- 두개의 문장을 구분자([SEP])를 넣어 구분하고 그 두 문장을 하나의 Segment로 지정하여 입력한다.

#### Position Embedding
- Token 순대로 인코딩을 한다.
