## Attention Mechanism
[참고](https://wikidocs.net/22893)

RNN(순환 신경망)에 기반한 모델에는 두가지 단점이 있는데,
- 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생한다.
- RNN의 고질적인 문제인 기울기 소실(Vanishing Gradient) 문제가 존재한다.

결국 이는 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상으로 나타났다. 이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 **Attention**이라는 기법이 생겨났다.

## Attention 기법
- 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다.
- 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 **집중(attention)**해서 보게 된다.
- Attention에는 여러 종류가 있다. 스코어 함수의 계산식에 차이가 있고 dot, location−base, scaled dot, general, concat 등 여러 방법이 있다.
