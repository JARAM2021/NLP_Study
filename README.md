# nlp_study
2021 NLP Study

## pre-training
선행학습. 이전에 사용되었던 모델의 정보를 입력받아 새로운 모델을 학습하는 Transfer Learning 에서 사전에 모델을 학습시키는 것을 말한다. NLP에서는 자연어를 기계가 처리할 수 있도록(특히 동음이의어, 다의어의 처리) 변환해 놓은 모델을 만드는 것이다.

## fine-tuning
기존의 학습되어 있는 모델을 기반으로 모델을 목적에 맞게 변형하고 이미 학습된 모델로부터 학습을 업데이트 하는 것이다.

## Word2Vec
워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현으로 변환한다.

- 희소 표현: 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법(원 핫 인코딩) 이렇게 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 한다.

- 밀집 표현: 밀집 표현은 벡터의 차원을 단어 집합의 크기로 상정하지 않고 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춘다.
벡터에 0과 1만이 아닌 실수가 들어간다.

- 워드 임베딩: 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding) 이라고 한다.

- 분산 표현: '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다' 라는 가정에서 비롯된 표현 방법으로 이 가설을 이용하여 단어들의 셋을 학습하고, 벡터에 단어의 의미를 여러 차원에 분산하여 표현한다.

- Word2Vec: 분산 표현의 학습 방법중에 하나이다.

## TF-IDF
- TF: Term Frequency - 문서에서 해당 단어의 빈도를 나타냄.
- IDF: Inverse Document Frequency - DF의 역수로 전체 문서에서 해당 단어가 있는지 나타냄.
- DF: 단어가 나타난 문서 수/문서 수.
- TF-IDF: TF와 IDF를 곱한 값으로 높을수록 해당 문서에서 자주 등장한다->중요성이 높다.

## Attention Mechanism
[참고](https://wikidocs.net/22893)

RNN(순환 신경망)에 기반한 모델에는 두가지 단점이 있는데,
- 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생한다.
- RNN의 고질적인 문제인 기울기 소실(Vanishing Gradient) 문제가 존재한다.

결국 이는 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상으로 나타났다. 이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 **Attention**이라는 기법이 생겨났다.

## Attention 기법
- 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다.
- 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 **집중(attention)**해서 보게 된다.
- Attention에는 여러 종류가 있다. 스코어 함수의 계산식에 차이가 있고 dot, location−base, scaled dot, general, concat 등 여러 방법이 있다.
